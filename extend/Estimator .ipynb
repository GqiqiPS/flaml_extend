{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f3f05a-d981-4570-8aa3-c7aa450b2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d2fa070-18dc-4dff-956c-4d1aa4e6daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from multiprocessing import  Pool\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    BertForTokenClassification,\n",
    "    BertForSequenceClassification\n",
    ")\n",
    "from transformers import (\n",
    "    DistilBertConfig,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForTokenClassification,\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a795d-e78f-4177-9e40-17add3feaaa4",
   "metadata": {},
   "source": [
    "## define teacher and student Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebe8373f-c397-4d43-92c4-048b8dfe34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_class = BertForSequenceClassification\n",
    "student_class = DistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058abf2e-3c37-4b1a-a3dc-fbbce468608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"sst2\"\n",
    "model_checkpoint = \"distilbert-base\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06f6ff6-a2d8-4c86-9a54-092b86309477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2021 23:13:21 - WARNING - datasets.builder - PID: 12594 -  Reusing dataset glue (/home/zftest/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a873c4489eb846808ece50293aa9cc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bff58a5-037d-41da-ba4a-ef9f05c7242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = DatasetDict()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fa0b54-be11-4e4a-8a74-ee33fa6edcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2053637d-962e-4677-a5bf-92c1295286c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>has some unnecessary parts and</td>\n",
       "      <td>negative</td>\n",
       "      <td>59298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curiously , super troopers suffers because it does n't have enough vices to merit its 103-minute length .</td>\n",
       "      <td>negative</td>\n",
       "      <td>51223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spare dialogue and</td>\n",
       "      <td>positive</td>\n",
       "      <td>15292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>effecting change</td>\n",
       "      <td>positive</td>\n",
       "      <td>62075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beautifully filmed and well acted ... but admittedly problematic in its narrative specifics .</td>\n",
       "      <td>positive</td>\n",
       "      <td>24161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i 'm not sure which is worse : the poor acting by the ensemble cast , the flat dialogue by vincent r. nebrida or the gutless direction by laurice guillen .</td>\n",
       "      <td>negative</td>\n",
       "      <td>29612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-- that you should never forget</td>\n",
       "      <td>positive</td>\n",
       "      <td>1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>are canny and spiced with irony</td>\n",
       "      <td>positive</td>\n",
       "      <td>15001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what 's invigorating about</td>\n",
       "      <td>positive</td>\n",
       "      <td>32552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the power of the huston performance</td>\n",
       "      <td>positive</td>\n",
       "      <td>54947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee90456-20a1-4a15-8aa3-191d9345a87a",
   "metadata": {},
   "source": [
    "## data prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0a5fb-a53b-4cd0-92c1-3b340ed84cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e79a95f-86b9-46a5-b1ec-2f0c3fbcc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "bos = tokenizer.special_tokens_map[\"cls_token\"]  # `[CLS]`\n",
    "sep = tokenizer.special_tokens_map[\"sep_token\"]  # `[SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d52512-0254-4b43-998d-576dc4316033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68442ed-d2fd-4344-bc86-668ff9809a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ds_p(df):\n",
    "    df['token_ids'] = df['sentence'].apply(\n",
    "        lambda x:np.array(tokenizer.encode(x, add_special_tokens=False))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412e053-11df-4990-b46c-ab5a53ac2e84",
   "metadata": {},
   "source": [
    "## X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddfbbca-a7c6-47f7-b840-21a4a6e5cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label  idx  \\\n",
      "0       hide new secretions from the parental units       0    0   \n",
      "1               contains no wit , only labored gags       0    1   \n",
      "2  that loves its characters and communicates som...      1    2   \n",
      "3  remains utterly satisfied to remain the same t...      0    3   \n",
      "4  on the worst revenge-of-the-nerds clichÃ©s the ...      0    4   \n",
      "\n",
      "                                           token_ids  \n",
      "0  [5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197]  \n",
      "1  [3397, 2053, 15966, 1010, 2069, 4450, 2098, 18...  \n",
      "2  [2008, 7459, 2049, 3494, 1998, 10639, 2015, 22...  \n",
      "3  [3464, 12580, 8510, 2000, 3961, 1996, 2168, 2802]  \n",
      "4  [2006, 1996, 5409, 7195, 1011, 1997, 1011, 199...  \n"
     ]
    }
   ],
   "source": [
    "df = parallelize_dataframe(pd.DataFrame(dataset[\"train\"]),\n",
    "                            tokenize_ds_p, \n",
    "                            n_cores=40\n",
    "                           )\n",
    "print(df.head())\n",
    "X_train = df[[\"token_ids\"]]\n",
    "y_train = df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70712238-7515-4f31-b03b-756ff45e0972",
   "metadata": {},
   "source": [
    "## X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5c384e-a66b-4ccb-8c61-4e37ce502aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label  idx  \\\n",
      "0    it 's a charming and often affecting journey .       1    0   \n",
      "1                 unflinchingly bleak and desperate       0    1   \n",
      "2  allows us to hope that nolan is poised to emba...      1    2   \n",
      "3  the acting , costumes , music , cinematography...      1    3   \n",
      "4                  it 's slow -- very , very slow .       0    4   \n",
      "\n",
      "                                           token_ids  \n",
      "0  [2009, 1005, 1055, 1037, 11951, 1998, 2411, 12...  \n",
      "1  [4895, 10258, 2378, 8450, 2135, 21657, 1998, 7...  \n",
      "2  [4473, 2149, 2000, 3246, 2008, 13401, 2003, 22...  \n",
      "3  [1996, 3772, 1010, 12703, 1010, 2189, 1010, 16...  \n",
      "4  [2009, 1005, 1055, 4030, 1011, 1011, 2200, 101...  \n"
     ]
    }
   ],
   "source": [
    "df = parallelize_dataframe(pd.DataFrame(dataset[\"validation\"]),\n",
    "                            tokenize_ds_p, \n",
    "                            n_cores=40\n",
    "                           )\n",
    "print(df.head())\n",
    "X_test = df[[\"token_ids\"]]\n",
    "y_test = df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45351dfc-d2ab-4cfa-a8b1-7c05203ee15f",
   "metadata": {},
   "source": [
    "## Teacher (from pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13f7bf26-0727-40f6-ae53-a333091a73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "teacher_name = \"bert-base-uncased\"\n",
    "teacher_type = \"bert\"\n",
    "student_type = \"distilbert\"\n",
    "teacher = teacher_class.from_pretrained(teacher_name, output_hidden_states=True)\n",
    "\n",
    "# \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71033b3-463b-4f54-b136-a6f1e64e6310",
   "metadata": {},
   "source": [
    "## Student (naive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d3ce4211-038c-4319-8411-4a887d44725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "student_config = \"conf/distilbert-base-uncased.json\"\n",
    "stu_architecture_config = DistilBertConfig.from_pretrained(student_config)\n",
    "print(stu_architecture_config)\n",
    "student_pretrained_weights = None\n",
    "student = student_class(stu_architecture_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53705d02-35b0-428e-8f10-bc886653305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert student.config.vocab_size == teacher.config.vocab_size\n",
    "assert student.config.hidden_size == teacher.config.hidden_size\n",
    "assert student.config.max_position_embeddings == teacher.config.max_position_embeddings\n",
    "\n",
    "student_config = student.config\n",
    "vocab_size = student.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52f331-29f5-4c3e-b2d2-745fcebc71f3",
   "metadata": {},
   "source": [
    "## Train distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c4532cc-082e-450e-8e77-8750c2004a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 5342,  2047,  3595,  8496,  2013,  1996, 18643,  3197]),\n",
       "       array([ 3397,  2053, 15966,  1010,  2069,  4450,  2098, 18201,  2015]),\n",
       "       array([ 2008,  7459,  2049,  3494,  1998, 10639,  2015,  2242,  2738,\n",
       "        3376,  2055,  2529,  3267]),\n",
       "       ...,\n",
       "       array([ 2012, 10910,  1996, 10754,  1010,  4306,  1011, 24820,  3289,\n",
       "        2009,  4520,  2005,  2993]),\n",
       "       array([ 1037,  5776, 13972]),\n",
       "       array([ 2023,  2047, 23769,  2571,  1997,  5005,  1010, 26865,  1998,\n",
       "       28072,  2442,  2022,  1037,  3809, 20127,  2005,  1996,  2516,\n",
       "        1012])], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"token_ids\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5314ea35-091b-4f35-8c0a-39ecccde34e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n",
      "48452\n"
     ]
    }
   ],
   "source": [
    "dat = X_train[\"token_ids\"].values\n",
    "print(len(dat))\n",
    "indices = np.array([len(x) for x in dat]) > 4\n",
    "dat = dat[indices]\n",
    "\n",
    "indices = np.array([len(x) for x in dat]) < 50\n",
    "dat = dat[indices]\n",
    "print(len(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fb7e984-7c96-4ec7-90bc-b4b27b8a00a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2021 23:21:29 - INFO - utils - PID: 12594 -  Remove 147 too long (>50 tokens) sequences.\n",
      "12/03/2021 23:21:29 - INFO - utils - PID: 12594 -  Remove 13365 too short (<=3 tokens) sequences.\n",
      "12/03/2021 23:21:29 - INFO - utils - PID: 12594 -  53837 sequences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from lm_seqs_dataset import LmSeqsDataset\n",
    "dataset = LmSeqsDataset(X_train[\"token_ids\"].values,\n",
    "                        y_train.values,\n",
    "                        max_model_input_size=50,\n",
    "                        min_model_input_size=3\n",
    "                       )\n",
    "\n",
    "sampler = RandomSampler(dataset)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset,\n",
    "                        batch_size=3,\n",
    "                        # batch_sampler=sampler,\n",
    "                        collate_fn=dataset.batch_sequences\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b95f786f-0824-4e09-8d74-a9a2d8cf5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import math\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cb198bf1-fdc9-4488-914b-9b139a9ac586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "n_epoch = 3\n",
    "gradient_accumulation_steps = 2\n",
    "temperature = 2.0\n",
    "alpha_ce = 0.5\n",
    "alpha_clm =0.5\n",
    "alpha_mse = 1e-3\n",
    "alpha_ca = 0.1\n",
    "learning_rate = 1e-2\n",
    "adam_epsilon = 1e-08\n",
    "weight_decay = 0.0\n",
    "warmup_prop = 0.05\n",
    "\n",
    "ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "lm_loss_fct = nn.CrossEntropyLoss()\n",
    "mse_loss_fct = nn.MSELoss(reduction=\"sum\")\n",
    "cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "num_steps_epoch = len(dataloader)\n",
    "num_train_optimization_steps = (\n",
    "    int(num_steps_epoch / gradient_accumulation_steps * n_epoch) + 1\n",
    ")\n",
    "warmup_steps = math.ceil(num_train_optimization_steps * warmup_prop)\n",
    "\n",
    "\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in student.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in student.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate,\n",
    "    eps=adam_epsilon,\n",
    "    betas=(0.9, 0.98)\n",
    "    )\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_train_optimization_steps\n",
    ")\n",
    "\n",
    "n_total_iter = 0\n",
    "for _ in range(n_epoch):\n",
    "    n_iter = 0\n",
    "    for batch in dataloader:\n",
    "        student_outputs = student(batch[0],output_hidden_states=True)\n",
    "        teacher_outputs = teacher(batch[0],output_hidden_states=True)\n",
    "\n",
    "\n",
    "        s_logits, s_h = student_outputs[\"logits\"], student_outputs[\"hidden_states\"]\n",
    "        t_logits, t_h = teacher_outputs[\"logits\"], teacher_outputs[\"hidden_states\"]\n",
    "\n",
    "        assert s_logits.size() == t_logits.size()\n",
    "\n",
    "\n",
    "        loss_ce = (\n",
    "            ce_loss_fct(\n",
    "                nn.functional.log_softmax(s_logits / temperature, dim=-1),\n",
    "                nn.functional.softmax(t_logits / temperature, dim=-1),\n",
    "            )\n",
    "            * (temperature) ** 2\n",
    "        )\n",
    "        loss = alpha_ce * loss_ce\n",
    "\n",
    "        loss_clm = lm_loss_fct(s_logits, batch[1])\n",
    "\n",
    "        loss += alpha_clm * loss_clm\n",
    "\n",
    "        dim = s_h[-1].shape[0]\n",
    "        slh = s_h[-1].view(dim,-1)\n",
    "        tlh = t_h[-1].view(dim,-1)\n",
    "        loss_cos = cosine_loss_fct(slh,\n",
    "                                   tlh,\n",
    "                                   target = slh.new(slh.size(0)).fill_(1)\n",
    "                                  )\n",
    "        loss += alpha_ca * loss_cos\n",
    "\n",
    "        # Check for NaN\n",
    "        if (loss != loss).data.any():\n",
    "            logger.error(\"NaN detected\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss.backward()\n",
    "        n_iter += 1\n",
    "        n_total_iter += 1\n",
    "\n",
    "\n",
    "        if n_iter % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "63dfd42b-523b-4af7-b371-530216aa2eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6eeef78c-2920-4f8e-874d-baf936106828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 13, 768])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_h[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "62bec4ad-c0ab-4a64-8fc8-0db874896ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 13, 768])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_h[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e12cd5-853f-481c-9e99-361b1f620632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py(cu101)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}